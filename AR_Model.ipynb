{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM95By19fJhf1xOmpY2ROPK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyna478/Paz/blob/main/AR_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_TCTHD6vYmxa"
      },
      "outputs": [],
      "source": [
        "unique_location_types = df['Location'].unique()\n",
        "print(unique_location_types)\n",
        "df2 = pd.read_csv('/content/financial_anomaly_data.csv')\n",
        "df2['TransactionType'] = df2['TransactionType'].fillna('Unknown')\n",
        "df2['Location'] = df2['Location'].fillna('Unknown')\n",
        "df_encoded = pd.get_dummies(df2, columns=['TransactionType'], prefix='TransactionType')\n",
        "df_encoded = pd.get_dummies(df2, columns=['Location', 'TransactionType'], prefix=['Location', 'TransactionType'])\n",
        "print(df_encoded.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_encoded.isnull().sum())\n",
        "data1 = df_encoded.dropna()\n",
        "print(data1.isnull().sum())"
      ],
      "metadata": {
        "id": "zJBtQH2qZDRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Dropout, Conv1DTranspose\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.utils import Sequence\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def generate_time_series(num_samples, time_steps, num_features):\n",
        "    \"\"\"Generate synthetic time-series data.\"\"\"\n",
        "    data = np.random.randn(num_samples, time_steps, num_features).astype(np.float32)\n",
        "    return data\n",
        "\n",
        "num_samples = 10000\n",
        "time_steps = 20\n",
        "num_features = 128\n",
        "\n",
        "data = data1\n",
        "\n",
        "train_data = data[:8000]\n",
        "val_data = data[8000:]\n",
        "\n",
        "class TimeSeriesGenerator(Sequence):\n",
        "    \"\"\"Custom data generator for time-series data.\"\"\"\n",
        "    def __init__(self, data, batch_size=300):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch = self.data[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        return batch, batch\n",
        "batch_size = 300\n",
        "train_generator = TimeSeriesGenerator(train_data, batch_size)\n",
        "val_generator = TimeSeriesGenerator(val_data, batch_size)\n",
        "\n",
        "def build_autoencoder(input_shape):\n",
        "    \"\"\"Build a convolutional autoencoder.\"\"\"\n",
        "    model = Sequential([\n",
        "        # Encoder\n",
        "        Conv1D(32, kernel_size=3, activation='relu', padding='same', input_shape=input_shape),\n",
        "        Dropout(0.2),\n",
        "        Conv1D(16, kernel_size=3, activation='relu', padding='same'),\n",
        "\n",
        "        # Decoder\n",
        "        Conv1DTranspose(16, kernel_size=3, activation='relu', padding='same'),\n",
        "        Dropout(0.2),\n",
        "        Conv1DTranspose(32, kernel_size=3, activation='relu', padding='same'),\n",
        "        Conv1DTranspose(num_features, kernel_size=3, padding='same')  # Output layer (no activation)\n",
        "    ])\n",
        "    return model\n",
        "input_shape = (time_steps, num_features)\n",
        "autoencoder = build_autoencoder(input_shape)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "autoencoder.summary()\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "epochs = 40\n",
        "steps_per_epoch = len(train_generator)\n",
        "validation_steps = len(val_generator)\n",
        "history = autoencoder.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[reduce_lr, early_stopping]\n",
        ")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "sample = val_data[:1]\n",
        "reconstructed = autoencoder.predict(sample)\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(sample[0, :, 0], label='Original')\n",
        "plt.plot(reconstructed[0, :, 0], label='Reconstructed')\n",
        "plt.title('Original vs Reconstructed Time-Series')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Feature Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XtrWe-JRZKWF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from tensorflow.keras.utils import Sequence\n",
        "\n",
        "print(data1.head())\n",
        "print(data1.isnull().sum())\n",
        "print(data1.dtypes)\n",
        "\n",
        "time_series_data = data1[['Amount', 'Location_London', 'Location_Los Angeles', 'Location_New York', 'Location_San Francisco', 'Location_Tokyo', 'TransactionType_Purchase', 'TransactionType_Transfer', 'TransactionType_Unknown', 'TransactionType_Withdrawal']]\n",
        "\n",
        "\n",
        "time_series_data = time_series_data.to_numpy()\n",
        "\n",
        "\n",
        "time_steps = 20\n",
        "num_features = time_series_data.shape[1]\n",
        "num_samples = len(time_series_data) // time_steps\n",
        "\n",
        "time_series_data = time_series_data[:num_samples * time_steps]\n",
        "time_series_data = time_series_data.reshape((num_samples, time_steps, num_features))\n",
        "\n",
        "print(f\"Reshaped data shape: {time_series_data.shape}\")\n",
        "\n",
        "class TimeSeriesGenerator(Sequence):\n",
        "    \"\"\"Custom data generator for time-series data.\"\"\"\n",
        "    def __init__(self, data, batch_size=300):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.data) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.data[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        batch_y = batch_x\n",
        "        return batch_x, batch_y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        \"\"\"Shuffle data at the end of each epoch.\"\"\"\n",
        "        np.random.shuffle(self.data)\n",
        "\n",
        "train_data = time_series_data[:8000]\n",
        "val_data = time_series_data[8000:]\n",
        "batch_size = 300\n",
        "train_generator = TimeSeriesGenerator(train_data, batch_size)\n",
        "val_generator = TimeSeriesGenerator(val_data, batch_size)\n",
        "\n",
        "batch_x, batch_y = train_generator[0]\n",
        "print(f\"Batch shape (inputs): {batch_x.shape}\")\n",
        "print(f\"Batch shape (targets): {batch_y.shape}\")"
      ],
      "metadata": {
        "id": "oqwDaAP8ZpCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Conv1DTranspose, Dropout, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def build_autoencoder(input_shape):\n",
        "    \"\"\"Build a convolutional autoencoder.\"\"\"\n",
        "    model = Sequential([\n",
        "        # Encoder\n",
        "        Conv1D(32, kernel_size=3, strides=1, padding='same', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Conv1D(16, kernel_size=3, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.2),\n",
        "\n",
        "\n",
        "        Conv1DTranspose(16, kernel_size=3, strides=2, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Conv1DTranspose(32, kernel_size=3, strides=1, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.2),\n",
        "\n",
        "\n",
        "        Conv1DTranspose(input_shape[-1], kernel_size=3, padding='same')  # Reconstruct input\n",
        "    ])\n",
        "    return model\n",
        "input_shape = (20, 10)\n",
        "\n",
        "autoencoder = build_autoencoder(input_shape)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "autoencoder.summary()\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "epochs = 40\n",
        "steps_per_epoch = len(train_generator)\n",
        "validation_steps = len(val_generator)\n",
        "\n",
        "history = autoencoder.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[reduce_lr, early_stopping]\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "sample = val_data[:1]  # Take one sample\n",
        "reconstructed = autoencoder.predict(sample)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(sample[0, :, 0], label='Original')\n",
        "plt.plot(reconstructed[0, :, 0], label='Reconstructed')\n",
        "plt.title('Original vs Reconstructed Time-Series')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Feature Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WsIme6tuZ6AB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, Conv1DTranspose, Dropout, BatchNormalization, LeakyReLU\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "time_series_data = data1[['Amount', 'Location_London', 'Location_Los Angeles',\n",
        "                          'Location_New York', 'Location_San Francisco', 'Location_Tokyo',\n",
        "                          'TransactionType_Purchase', 'TransactionType_Transfer',\n",
        "                          'TransactionType_Unknown', 'TransactionType_Withdrawal']]\n",
        "\n",
        "print(\"Data types before conversion:\", time_series_data.dtypes)\n",
        "\n",
        "time_series_data = time_series_data.astype('float32')\n",
        "if time_series_data.isnull().sum().sum() > 0:\n",
        "    print(\"Found NaN values. Replacing with zeros.\")\n",
        "    time_series_data = time_series_data.fillna(0)\n",
        "time_series_data = time_series_data.to_numpy()\n",
        "\n",
        "print(\"Data shape before reshaping:\", time_series_data.shape)\n",
        "print(\"Data min/max values:\", np.min(time_series_data), np.max(time_series_data))\n",
        "print(\"Data contains NaN:\", np.isnan(time_series_data).any())\n",
        "print(\"Data contains Inf:\", np.isinf(time_series_data).any())\n",
        "\n",
        "time_steps = 20\n",
        "num_features = time_series_data.shape[1]\n",
        "num_samples = len(time_series_data) // time_steps\n",
        "\n",
        "# Reshape the data\n",
        "time_series_data = time_series_data[:num_samples * time_steps]\n",
        "time_series_data = time_series_data.reshape((num_samples, time_steps, num_features))\n",
        "print(f\"Reshaped data shape: {time_series_data.shape}\")\n",
        "class TimeSeriesGenerator(Sequence):\n",
        "\n",
        "    def __init__(self, data, batch_size=300):\n",
        "        self.data = data\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.ceil(len(self.data) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        batch_x = self.data[idx * self.batch_size : (idx + 1) * self.batch_size]\n",
        "        batch_y = batch_x\n",
        "        return batch_x, batch_y\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "\n",
        "        indices = np.arange(len(self.data))\n",
        "        np.random.shuffle(indices)\n",
        "        self.data = self.data[indices]\n",
        "train_size = int(0.8 * num_samples)\n",
        "train_data = time_series_data[:train_size]\n",
        "val_data = time_series_data[train_size:]\n",
        "\n",
        "print(f\"Training data shape: {train_data.shape}\")\n",
        "print(f\"Validation data shape: {val_data.shape}\")\n",
        "batch_size = min(300, len(train_data))\n",
        "train_generator = TimeSeriesGenerator(train_data, batch_size)\n",
        "val_generator = TimeSeriesGenerator(val_data, batch_size)\n",
        "\n",
        "# Verify the generator - this is important to check for errors\n",
        "try:\n",
        "    batch_x, batch_y = train_generator[0]\n",
        "    print(f\"Batch shape (inputs): {batch_x.shape}\")\n",
        "    print(f\"Batch shape (targets): {batch_y.shape}\")\n",
        "    print(f\"Data type of batch: {batch_x.dtype}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error in generator: {e}\")\n",
        "\n",
        "# Step 3: Build the Autoencoder Model\n",
        "def build_autoencoder(input_shape):\n",
        "\n",
        "    model = Sequential([\n",
        "\n",
        "        Conv1D(32, kernel_size=3, strides=1, padding='same', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Conv1D(16, kernel_size=3, strides=1, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.2),\n",
        "        Conv1DTranspose(16, kernel_size=3, strides=1, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.2),\n",
        "\n",
        "        Conv1DTranspose(32, kernel_size=3, strides=1, padding='same'),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.1),\n",
        "        Dropout(0.2),\n",
        "        Conv1DTranspose(input_shape[-1], kernel_size=3, padding='same')\n",
        "    ])\n",
        "    return model\n",
        "input_shape = (time_steps, num_features)\n",
        "print(f\"Model input shape: {input_shape}\")\n",
        "autoencoder = build_autoencoder(input_shape)\n",
        "autoencoder.compile(optimizer='adam', loss='mse')\n",
        "autoencoder.summary()\n",
        "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-6)\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "\n",
        "epochs = 40\n",
        "steps_per_epoch = len(train_generator)\n",
        "validation_steps = len(val_generator)\n",
        "\n",
        "history = autoencoder.fit(\n",
        "    train_generator,\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=steps_per_epoch,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=validation_steps,\n",
        "    callbacks=[reduce_lr, early_stopping]\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "sample = val_data[:1]\n",
        "reconstructed = autoencoder.predict(sample)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(sample[0, :, 0], label='Original')\n",
        "plt.plot(reconstructed[0, :, 0], label='Reconstructed')\n",
        "plt.title('Original vs Reconstructed Time-Series (First Feature)')\n",
        "plt.xlabel('Time Steps')\n",
        "plt.ylabel('Feature Value')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "def detect_anomalies(model, data, threshold_multiplier=3.0):\n",
        "\n",
        "    reconstructions = model.predict(data)\n",
        "    mse = np.mean(np.square(data - reconstructions), axis=(1, 2))\n",
        "    threshold = np.mean(mse) + threshold_multiplier * np.std(mse)\n",
        "    anomalies = mse > threshold\n",
        "\n",
        "    return mse, anomalies, threshold\n",
        "anomaly_scores, anomalies, threshold = detect_anomalies(autoencoder, val_data)\n",
        "\n",
        "print(f\"Number of anomalies detected: {np.sum(anomalies)}\")\n",
        "print(f\"Percentage of anomalies: {np.sum(anomalies) / len(anomalies) * 100:.2f}%\")\n",
        "print(f\"Anomaly threshold: {threshold:.6f}\")\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(anomaly_scores)\n",
        "plt.axhline(y=threshold, color='r', linestyle='-', label=f'Threshold ({threshold:.6f})')\n",
        "plt.title('Anomaly Scores')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('Reconstruction Error (MSE)')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "DCn-EmfyaMGj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Input, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_curve, auc\n",
        "import seaborn as sns\n",
        "\n",
        "def prepare_data(data1):\n",
        "\n",
        "    time_series_data = data1[['Amount', 'Location_London', 'Location_Los Angeles',\n",
        "                             'Location_New York', 'Location_San Francisco', 'Location_Tokyo',\n",
        "                             'TransactionType_Purchase', 'TransactionType_Transfer',\n",
        "                             'TransactionType_Unknown', 'TransactionType_Withdrawal']]\n",
        "    time_series_data = time_series_data.fillna(0)\n",
        "    raw_data = time_series_data.copy()\n",
        "    scaler = StandardScaler()\n",
        "    time_series_data_scaled = scaler.fit_transform(time_series_data)\n",
        "    time_series_data_scaled = time_series_data_scaled.astype(np.float32)\n",
        "    time_steps = 20\n",
        "    num_features = time_series_data_scaled.shape[1]\n",
        "    num_samples = len(time_series_data_scaled) // time_steps\n",
        "\n",
        "    time_series_data_scaled = time_series_data_scaled[:num_samples * time_steps]\n",
        "    shaped_data = time_series_data_scaled.reshape((num_samples, time_steps, num_features))\n",
        "\n",
        "    print(f\"Data shape after reshaping: {shaped_data.shape}\")\n",
        "\n",
        "    return shaped_data, scaler, raw_data, time_steps, num_features\n",
        "def build_lstm_autoencoder(input_shape, encoding_dim=8):\n",
        "\n",
        "    inputs = Input(shape=input_shape)\n",
        "    x = LSTM(64, activation='relu', return_sequences=True)(inputs)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = LSTM(32, activation='relu', return_sequences=False)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    encoded = Dense(encoding_dim, activation='relu')(x)\n",
        "    x = Dense(32, activation='relu')(encoded)\n",
        "    x = RepeatVector(input_shape[0])(x)\n",
        "\n",
        "    x = LSTM(32, activation='relu', return_sequences=True)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "\n",
        "    x = LSTM(64, activation='relu', return_sequences=True)(x)\n",
        "    x = Dropout(0.2)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    outputs = TimeDistributed(Dense(input_shape[1]))(x)\n",
        "    autoencoder = Model(inputs=inputs, outputs=outputs)\n",
        "    optimizer = Adam(learning_rate=0.001)\n",
        "    autoencoder.compile(optimizer=optimizer, loss='mse')\n",
        "\n",
        "    return autoencoder\n",
        "\n",
        "def train_model(autoencoder, train_data, val_data, epochs=100, batch_size=64):\n",
        "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)\n",
        "    checkpoint = ModelCheckpoint('best_autoencoder.h5', monitor='val_loss', save_best_only=True, verbose=0)\n",
        "    history = autoencoder.fit(\n",
        "        train_data, train_data,\n",
        "        epochs=epochs,\n",
        "        batch_size=batch_size,\n",
        "        validation_data=(val_data, val_data),\n",
        "        callbacks=[reduce_lr, early_stopping, checkpoint],\n",
        "        shuffle=True,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "def detect_anomalies(model, data, threshold_percentile=95):\n",
        "    reconstructions = model.predict(data)\n",
        "    mse = np.mean(np.square(data - reconstructions), axis=(1, 2))\n",
        "    threshold = np.percentile(mse, threshold_percentile)\n",
        "    anomalies = mse > threshold\n",
        "\n",
        "    return mse, anomalies, threshold, reconstructions\n",
        "\n",
        "def plot_training_history(history):\n",
        "    \"\"\"Plot the training and validation loss\"\"\"\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(history.history['loss'], label='Training Loss')\n",
        "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "    plt.title('Model Loss During Training')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss (MSE)')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_reconstructions(original, reconstructed, sample_idx=0, feature_idx=0):\n",
        "\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.plot(original[sample_idx, :, feature_idx], 'b-', label='Original', linewidth=2)\n",
        "    plt.plot(reconstructed[sample_idx, :, feature_idx], 'r-', label='Reconstructed', linewidth=2)\n",
        "    plt.title(f'Original vs Reconstructed Time-Series (Sample {sample_idx}, Feature {feature_idx})')\n",
        "    plt.xlabel('Time Steps')\n",
        "    plt.ylabel('Normalized Feature Value')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "def plot_anomaly_scores(mse, threshold, anomalies=None):\n",
        "\n",
        "    plt.figure(figsize=(14, 7))\n",
        "    plt.plot(mse, 'b-', alpha=0.6, label='Reconstruction Error')\n",
        "    if anomalies is not None:\n",
        "        plt.scatter(np.where(anomalies)[0], mse[anomalies],\n",
        "                    color='red', alpha=0.7, s=50, label='Anomalies')\n",
        "\n",
        "    plt.axhline(y=threshold, color='r', linestyle='-',\n",
        "               label=f'Threshold ({threshold:.4f})')\n",
        "\n",
        "    plt.title('Anomaly Scores')\n",
        "    plt.xlabel('Sample Index')\n",
        "    plt.ylabel('Reconstruction Error (MSE)')\n",
        "    plt.yscale('log')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def plot_feature_importance(original, reconstructed):\n",
        "\n",
        "    feature_mse = np.mean(np.square(original - reconstructed), axis=(0, 1))\n",
        "    feature_names = ['Amount', 'London', 'Los Angeles', 'New York',\n",
        "                     'San Francisco', 'Tokyo', 'Purchase', 'Transfer',\n",
        "                     'Unknown', 'Withdrawal']\n",
        "    sorted_idx = np.argsort(feature_mse)[::-1]\n",
        "\n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.bar(range(len(feature_mse)), feature_mse[sorted_idx])\n",
        "    plt.xticks(range(len(feature_mse)), [feature_names[i] for i in sorted_idx], rotation=45)\n",
        "    plt.title('Feature Contribution to Reconstruction Error')\n",
        "    plt.xlabel('Features')\n",
        "    plt.ylabel('Mean Squared Error')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def run_fraud_detection(data1):\n",
        "    print(\"Preparing data...\")\n",
        "    shaped_data, scaler, raw_data, time_steps, num_features = prepare_data(data1)\n",
        "    train_size = int(0.8 * shaped_data.shape[0])\n",
        "    train_data = shaped_data[:train_size]\n",
        "    val_data = shaped_data[train_size:]\n",
        "\n",
        "    print(f\"Training data shape: {train_data.shape}\")\n",
        "    print(f\"Validation data shape: {val_data.shape}\")\n",
        "    print(\"Building LSTM autoencoder model...\")\n",
        "    input_shape = (time_steps, num_features)\n",
        "    autoencoder = build_lstm_autoencoder(input_shape)\n",
        "    autoencoder.summary()\n",
        "    print(\"Training model...\")\n",
        "    history = train_model(autoencoder, train_data, val_data)\n",
        "    print(\"Evaluating model...\")\n",
        "    plot_training_history(history)\n",
        "    mse, anomalies, threshold, reconstructions = detect_anomalies(autoencoder, val_data)\n",
        "    plot_reconstructions(val_data, reconstructions)\n",
        "    plot_anomaly_scores(mse, threshold, anomalies)\n",
        "    anomaly_percentage = np.mean(anomalies) * 100\n",
        "    print(f\"Number of anomalies detected: {np.sum(anomalies)} out of {len(anomalies)}\")\n",
        "    print(f\"Percentage of anomalies: {anomaly_percentage:.2f}%\")\n",
        "    print(f\"Anomaly threshold: {threshold:.6f}\")\n",
        "    plot_feature_importance(val_data, reconstructions)\n",
        "    return autoencoder, scaler, mse, anomalies, threshold\n",
        "\n",
        "def predict_anomalies(model, scaler, new_data, threshold, time_steps=20):\n",
        "\n",
        "    scaled_data = scaler.transform(new_data)\n",
        "\n",
        "\n",
        "    num_samples = len(scaled_data) // time_steps\n",
        "    if num_samples > 0:\n",
        "\n",
        "        scaled_data = scaled_data[:num_samples * time_steps]\n",
        "\n",
        "        shaped_data = scaled_data.reshape((num_samples, time_steps, scaled_data.shape[1]))\n",
        "\n",
        "\n",
        "        reconstructions = model.predict(shaped_data)\n",
        "\n",
        "\n",
        "        mse = np.mean(np.square(shaped_data - reconstructions), axis=(1, 2))\n",
        "\n",
        "\n",
        "        anomalies = mse > threshold\n",
        "\n",
        "\n",
        "        result = pd.DataFrame({\n",
        "            'reconstruction_error': mse,\n",
        "            'is_anomaly': anomalies\n",
        "        })\n",
        "\n",
        "        return result\n",
        "    else:\n",
        "        print(\"Not enough data points for a complete sequence\")\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "xbRO5o-da2Ht"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}