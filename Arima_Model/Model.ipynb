{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOq/05X8bBmNmNQAjS4+pzQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dyna478/Paz/blob/main/Arima_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6fyFMSp3MC2R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from pmdarima import auto_arima\n",
        "import numpy as np\n",
        "\n",
        "df1['Timestamp'] = pd.to_datetime(df['Timestamp'])\n",
        "\n",
        "df1 = df.sort_values('Timestamp')\n",
        "\n",
        "df_daily = df1.groupby(pd.Grouper(key='Timestamp', freq='D')).agg({'Amount': 'sum'}).reset_index()\n",
        "\n",
        "model = auto_arima(\n",
        "    df_daily['Amount'],\n",
        "    start_p=0,\n",
        "    start_q=0,\n",
        "    max_p=5,\n",
        "    max_q=5,\n",
        "    d=None,\n",
        "    seasonal=False,\n",
        "    trace=True,\n",
        "    error_action='ignore',\n",
        "    suppress_warnings=True,\n",
        "    stepwise=True\n",
        ")\n",
        "\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "model = ARIMA(df_daily['Amount'], order=(1, 0, 3))\n",
        "model_fit = model.fit()\n",
        "predictions = model_fit.predict(start=0, end=len(df_daily) - 1)\n",
        "errors = predictions - df_daily['Amount']\n",
        "\n",
        "mu = np.mean(errors)\n",
        "sigma = np.std(errors)\n",
        "threshold_upper = mu + 3 * sigma\n",
        "threshold_lower = mu - 3 * sigma\n"
      ],
      "metadata": {
        "id": "wZHbp1VjMNeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_daily['Anomaly'] = np.where(\n",
        "    (errors > threshold_upper) | (errors < threshold_lower),\n",
        "    1,\n",
        "    0\n",
        ")\n",
        "\n",
        "anomalies = df_daily[df_daily['Anomaly'] == 1]\n",
        "print(\"Anomalies detected:\")\n",
        "print(anomalies)"
      ],
      "metadata": {
        "id": "Yi8vXtQGMh8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.arima.model import ARIMA\n",
        "model = ARIMA(df_daily['Amount'], order=(1, 0, 3))\n",
        "model_fit = model.fit()\n",
        "predictions = model_fit.predict(start=0, end=len(df_daily) - 1)\n",
        "errors = predictions - df_daily['Amount']\n",
        "rolling_mean = errors.rolling(window=window_size, min_periods=1).mean()\n",
        "rolling_std = errors.rolling(window=window_size, min_periods=1).std()\n",
        "threshold_upper = rolling_mean + 2 * rolling_std\n",
        "threshold_lower = rolling_mean - 2 * rolling_std"
      ],
      "metadata": {
        "id": "LGaByE-yMyPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_daily['Anomaly'] = np.where(\n",
        "    (errors > threshold_upper) | (errors < threshold_lower),\n",
        "    1,\n",
        "    0\n",
        ")\n",
        "\n",
        "anomalies = df_daily[df_daily['Anomaly'] == 1]\n",
        "print(\"Anomalies detected:\")\n",
        "print(anomalies)\n",
        "model = ARIMA(df_daily['Amount'], order=(1, 0, 1))\n",
        "model_fit = model.fit()\n",
        "predictions = model_fit.predict(start=0, end=len(df_daily) - 1)\n",
        "errors = predictions - df_daily['Amount']\n",
        "threshold = 3 * errors.std()\n",
        "df_daily['Predicted_Anomaly'] = (errors.abs() > threshold).astype(int)\n"
      ],
      "metadata": {
        "id": "Z1qcbtIsNCKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "y_true = df_daily['Anomaly']\n",
        "y_pred = df_daily['Predicted_Anomaly']\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f_score = f1_score(y_true, y_pred)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F-Score: {f_score:.4f}\")"
      ],
      "metadata": {
        "id": "zm3-LoQ4NQXA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred)\n",
        "plt.figure(figsize=(6, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False,\n",
        "            xticklabels=['Normal', 'Anomaly'],\n",
        "            yticklabels=['Normal', 'Anomaly'])\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Actual')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "prkmW1-cNZ3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "\n",
        "y_probs = model_fit.predict(start=0, end=len(df_daily) - 1, typ='levels')  # Replace with your probability predictions\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_true, y_probs)\n",
        "auc_score = roc_auc_score(y_true, y_probs)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('ROC Curve')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2I1hqQ0FNol-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "precision_scores, recall_scores, f1_scores = [], [], []\n",
        "\n",
        "\n",
        "for train_index, test_index in tscv.split(df_daily):\n",
        "    train, test = df_daily.iloc[train_index], df_daily.iloc[test_index]\n",
        "    model = ARIMA(train['Amount'], order=(1, 0, 3))\n",
        "    model_fit = model.fit()\n",
        "    predictions = model_fit.predict(start=test.index[0], end=test.index[-1])\n",
        "    errors = predictions - test['Amount']\n",
        "    threshold = 3 * errors.std()\n",
        "    test['Predicted_Anomaly'] = (errors.abs() > threshold).astype(int)\n",
        "    precision = precision_score(test['Anomaly'], test['Predicted_Anomaly'])\n",
        "    recall = recall_score(test['Anomaly'], test['Predicted_Anomaly'])\n",
        "    f1 = f1_score(test['Anomaly'], test['Predicted_Anomaly'])\n",
        "    precision_scores.append(precision)\n",
        "    recall_scores.append(recall)\n",
        "    f1_scores.append(f1)\n",
        "\n",
        "# Print average metrics\n",
        "print(f\"Average Precision: {np.mean(precision_scores):.4f}\")\n",
        "print(f\"Average Recall: {np.mean(recall_scores):.4f}\")\n",
        "print(f\"Average F1-Score: {np.mean(f1_scores):.4f}\")"
      ],
      "metadata": {
        "id": "zbe4FqCKOQQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Fixing Issues"
      ],
      "metadata": {
        "id": "NILAFa14OkF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"True Labels Distribution:\")\n",
        "print(df_daily['Anomaly'].value_counts())\n",
        "print(\"Predicted Labels Distribution:\")\n",
        "print(df_daily['Predicted_Anomaly'].value_counts())\n",
        "print(\"Number of NaN values in Predicted_Anomaly:\", df_daily['Predicted_Anomaly'].isna().sum())\n",
        "df_daily['Predicted_Anomaly'] = df_daily['Predicted_Anomaly'].fillna(0).astype(int)\n",
        "df_daily = df_daily.dropna(subset=['Predicted_Anomaly'])\n"
      ],
      "metadata": {
        "id": "_aElkCMROYT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "d = 0\n",
        "print(\"Length of errors:\", len(errors))\n",
        "print(\"Length of df_daily:\", len(df_daily))\n",
        "\n",
        "predictions = model_fit.predict(start=0, end=len(df_daily) - 1)\n",
        "errors = predictions - df_daily['Amount']\n",
        "if d > 0:\n",
        "    predictions = np.concatenate([[np.nan] * d, predictions])\n",
        "    errors = np.concatenate([[np.nan] * d, errors])\n",
        "threshold = 2 * errors.std()\n",
        "df_daily['Predicted_Anomaly'] = (np.abs(errors) > threshold).astype(int)\n",
        "df_daily['Predicted_Anomaly'] = df_daily['Predicted_Anomaly'].fillna(0).astype(int)\n",
        "precision = precision_score(y_true, df_daily['Predicted_Anomaly'])\n",
        "recall = recall_score(y_true, df_daily['Predicted_Anomaly'])\n",
        "f_score = f1_score(y_true, df_daily['Predicted_Anomaly'])\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F-Score: {f_score:.4f}\")"
      ],
      "metadata": {
        "id": "YFaEVLSYOvdQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of NaN values in errors:\", np.isnan(errors).sum())\n",
        "print(\"Number of infinite values in errors:\", np.isinf(errors).sum())\n",
        "errors = np.nan_to_num(errors, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "threshold = 2 * errors.std()\n",
        "df_daily['Predicted_Anomaly'] = (np.abs(errors) > threshold).astype(int)"
      ],
      "metadata": {
        "id": "M8-kCULoO2y2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 2 * errors.std()\n",
        "df_daily['Predicted_Anomaly'] = (np.abs(errors) > threshold).astype(int)\n",
        "precision = precision_score(y_true, df_daily['Predicted_Anomaly'])\n",
        "recall = recall_score(y_true, df_daily['Predicted_Anomaly'])\n",
        "f_score = f1_score(y_true, df_daily['Predicted_Anomaly'])\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F-Score: {f_score:.4f}\")"
      ],
      "metadata": {
        "id": "GCsXV4FaO8pE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_daily['Timestamp'], df_daily['Amount'], label='Actual')\n",
        "plt.plot(df_daily['Timestamp'], predictions, label='Predicted')\n",
        "plt.scatter(df_daily[df_daily['Predicted_Anomaly'] == 1]['Timestamp'],\n",
        "            df_daily[df_daily['Predicted_Anomaly'] == 1]['Amount'],\n",
        "            color='red', label='Predicted Anomalies')\n",
        "plt.legend()\n",
        "plt.title('Actual vs Predicted Values with Anomalies')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Amount')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WRA1wjHkPHTY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df_daily, test_size=0.2, shuffle=False)\n",
        "\n",
        "model = ARIMA(train['Amount'], order=(1, 0, 3))\n",
        "model_fit = model.fit()\n",
        "predictions = model_fit.predict(start=test.index[0], end=test.index[-1])\n",
        "errors = predictions - test['Amount']\n",
        "\n",
        "threshold = 2 * errors.std()\n",
        "test['Predicted_Anomaly'] = (errors.abs() > threshold).astype(int)\n",
        "precision = precision_score(test['Anomaly'], test['Predicted_Anomaly'])\n",
        "recall = recall_score(test['Anomaly'], test['Predicted_Anomaly'])\n",
        "f_score = f1_score(test['Anomaly'], test['Predicted_Anomaly'])\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F-Score: {f_score:.4f}\")"
      ],
      "metadata": {
        "id": "dHmQzyBpPKrz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"True Labels Distribution:\")\n",
        "print(df_daily['Anomaly'].value_counts())\n",
        "print(\"Predicted Labels Distribution:\")\n",
        "print(df_daily['Predicted_Anomaly'].value_counts())\n",
        "threshold = 2 * errors.std()\n",
        "df_daily['Predicted_Anomaly'] = (np.abs(errors) > threshold).astype(int)\n",
        "precision = precision_score(y_true, df_daily['Predicted_Anomaly'])\n",
        "recall = recall_score(y_true, df_daily['Predicted_Anomaly'])\n",
        "f_score = f1_score(y_true, df_daily['Predicted_Anomaly'])\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F-Score: {f_score:.4f}\")\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(df_daily['Timestamp'], df_daily['Amount'], label='Actual')\n",
        "plt.plot(df_daily['Timestamp'], predictions, label='Predicted')\n",
        "plt.scatter(df_daily[df_daily['Predicted_Anomaly'] == 1]['Timestamp'],\n",
        "            df_daily[df_daily['Predicted_Anomaly'] == 1]['Amount'],\n",
        "            color='red', label='Predicted Anomalies')\n",
        "plt.legend()\n",
        "plt.title('Actual vs Predicted Values with Anomalies')\n",
        "plt.xlabel('Timestamp')\n",
        "plt.ylabel('Amount')\n",
        "plt.show()\n",
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(df_daily, test_size=0.2, shuffle=False)\n",
        "\n",
        "model = ARIMA(train['Amount'], order=(1, 0, 1))\n",
        "model_fit = model.fit()\n",
        "predictions = model_fit.predict(start=test.index[0], end=test.index[-1])\n",
        "errors = predictions - test['Amount']\n",
        "threshold = 2 * errors.std()\n",
        "test['Predicted_Anomaly'] = (np.abs(errors) > threshold).astype(int)\n",
        "precision = precision_score(test['Anomaly'], test['Predicted_Anomaly'])\n",
        "recall = recall_score(test['Anomaly'], test['Predicted_Anomaly'])\n",
        "f_score = f1_score(test['Anomaly'], test['Predicted_Anomaly'])\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F-Score: {f_score:.4f}\")"
      ],
      "metadata": {
        "id": "d0lbqosTPdmX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
